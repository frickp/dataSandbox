{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keep in mind\n",
    "- ask about use case. Red flag is if the algo is chosen without a specific problem\n",
    "- are the data ingestion/logging parts complete? If first data scientist, could be bottom of \"AI hierarchy of needs\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_fibonacci(val):\n",
    "    if val==0:\n",
    "        return 0\n",
    "    elif val==1:\n",
    "        return 1\n",
    "    else:\n",
    "        return get_fibonacci(val-1)+get_fibonacci(val-2)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_fibonacci(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def factorial(n):\n",
    "    if n<=0:\n",
    "        return 1\n",
    "    else:\n",
    "        return n * factorial(n-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "factorial(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regularization imposes penalty on objective function. Thus\n",
    "\n",
    "For example, on a simple regression with one coefficient:\n",
    "y - a_1*x + a_0 + lambda*sum(a_1^2)\n",
    "\n",
    "LASSO is a Laplacian prior and Ridge regression is a gaussian prior"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### My data science story\n",
    "Always sought to have quantitive understanding of the world. Physics degree. I wanted to learn something new, apply math/engineering to new domain (biology). Math modeling, data munging, statistical analysis, image processing. Working in relatively new domain (high-throughput imaging). I enjoyed \n",
    "\n",
    "In Stanford, I interviewed at 3 labs and chose to join the one with the best compute infrastructure. Audit ML course\n",
    "\n",
    "I like my job, great co-workers, and learning new stuff. What I'd like to get is more exposure to the lifecycle of a model. Often we just deliver our best implementation. Some issues with third-party software, or else, no iterative optimization\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SQL\n",
    "- leetcode/hackerrank\n",
    "- SQL Zoo (facebook recruiter says this is best)\n",
    "- http://www.w3schools.com/sql (good, but not challenging)\n",
    "- db.grussell.org/sql\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Binomial distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\binom{n}{k}\\quad p^k(1-p)^{(n-k)}$$\n",
    "Where \n",
    "$$\\binom{n}{k} = \\frac{n!}{k!(n-k)!}$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algos I know\n",
    "\n",
    "*PhD*\n",
    "- Linear model\n",
    "- t-test\n",
    "- KS-test\n",
    "- ordinary differential equations\n",
    "- developing analytics/models in new domain\n",
    "- correlation test\n",
    "\n",
    "*Stanford*\n",
    "- data processing on remote cluster\n",
    "    - python\n",
    "    - bash\n",
    "- project management\n",
    "    - intiate collaborations\n",
    "    - direct technician\n",
    "    - report findings in scientific meetings\n",
    "\n",
    "*AT&T*\n",
    "- PCA\n",
    "- resampling\n",
    "- random forest\n",
    "- neural network\n",
    "- tSNE\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Example analysis:\n",
    "From joined data (third party- and internal), find zip codes to target. First, feature engineering. Bucket data. Transform using PCA to keep only salient variance. Compute distances (I think it was cosine similarity). Do tSNE to map to low dimensional space. Do DB-scan. Examine the features most over-under represented in features. Display zip codes on map out zip codes to check quality of clusters. \n",
    "\n",
    "How did I get higher-than-expected values? Keep data indices, so can cluster using principal components, but interpret by using the normalized variables. Variables were standardized independent of clusters. Then, use the cluster labels as a mask and compute the mean value for each variable. Then sort by the absolute value to find best variables.\n",
    "\n",
    "Do surveys to determine effectiveness of campaign. After the fact, did chi-squared test to find targetable variables. How did this work? People gave propensity 1–10, map this to suspeptible (4–7) or not (Thus, categorical variables). Then do a chi-squared test to test for independence. Null hypothesis is that there is no significant difference between susceptible/non–susceptible determine if the variable was significantly different. Also, power analysis to check the number of surveys needed to achieve a signitifant result.\n",
    "\n",
    "Outcome: achieved success defined by our business client. Got bonus and recognition for work.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### merge sort/bubble sort\n",
    "sorted list from two sorted lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def bubble_sort(arr):\n",
    "    for i_1 in range(len(arr)):\n",
    "        for i_2 in range(len(arr)-1):\n",
    "            if arr[i_2] > arr[i_2 + 1]:\n",
    "                temp = arr[i_2 + 1]\n",
    "                arr[i_2 + 1] = arr[i_2]\n",
    "                arr[i_2] = temp\n",
    "    return arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 1, 2, 5, 6, 9]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bubble_sort([9,1,5,2,6,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sorted_union(srt_1,srt_2):\n",
    "    result = [0]*(len(srt_1)+len(srt_2))\n",
    "    last_1 = len(srt_1) - 1\n",
    "    last_2 = len(srt_2) - 1\n",
    "    i, j, k = 0, 0, 0\n",
    "    while (i < last_1) & (j < last_2):\n",
    "        if srt_1[i] < srt_2[j]:\n",
    "            result[k] = srt_1[i]\n",
    "            i += 1\n",
    "            k += 1\n",
    "        else:\n",
    "            result[k] = srt_2[j]\n",
    "            j += 1\n",
    "            k += 1\n",
    "    while i <= last_1:\n",
    "        result[k] = srt_1[i]\n",
    "        k += 1\n",
    "        i += 1\n",
    "    while j <= last_2:\n",
    "        result[k] = srt_2[j]\n",
    "        k += 1\n",
    "        j += 1\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 3, 4, 5, 7, 6, 8]"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted_union([1,3,5,7],[4,6,8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 2, 3, 3, 3, 4, 4, 5]"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted_union([1,2,3,3],[2,3,4,4,5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A/B testing? Binomial significance test?\n",
    "\n",
    "[This](https://www.lunametrics.com/blog/2014/07/01/statistical-significance-test/) argues for using chi-squared test for A/B testing. Seems alright, but it is a _rate_, not counts that are important. This would be helpful for Probably Z-test in quora answer is best. [This](http://soc.utah.edu/sociology3112/chi-square.php) is a nice explanation on chi-squared tests. Null is that there is no relationship the variables\n",
    "\n",
    "[This](https://www.quora.com/What-statistical-test-should-be-used-for-an-A-B-test-on-conversion-rates#) looks pretty good.\n",
    "\n",
    "Let me try to summarize on-the-fly. Say you want to test a different layout on your website. The way you would test this would be to run an A/B test. First, users would be randomized to either control or treatment. The metric to quantify would be ctr (clickthrough rate). Either a user will click or not (0 or 1 per user), and this can be modeled by the binomial distribution, where the expected value is $$p(click)/(p(click)+p(no_click))$$ First, we define the null hypothesis, which is that $$ctrTreatment <= ctrControl$$. Or alternatively, $$ctrTreatment - ctrControl <= 0$$ We set a significance threshold of p = 0.05. In this case, I choose a one-tailed test. The distribution of proportions will be binomially distributed, but since we have a large number of observations, we can assume that the distribution of ctrTreatment - ctrControl is normal (because of the central limit theorem). Therefore, we use a Z-test. Calculate the z-statistic. Z = (mean_obs - mean_null)/(stdev/sqrt(n)) = (mean_obs/std_error). Then check if Z > 1.65. If so, this is significant at 95% confidence level.\n",
    "\n",
    "To compute the standard error, we will use stdev/sqrt(n). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### re-watch the kahn academy significance testing videos.\n",
    "[This one](https://www.khanacademy.org/math/statistics-probability/significance-tests-one-sample/tests-about-population-proportion/v/large-sample-proportion-hypothesis-testing) on testing proportions is good."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### T-Test\n",
    "p–value: probability of getting results this extreme or more given that the null hypothesis is true\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://medium.com/airbnb-engineering/experiments-at-airbnb-e2db3abf39e7\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### classification eval metrics\n",
    "_precision_:\n",
    "- TP/(TP + FP)\n",
    "- I predicted some values as True. What proportion of these predictions are actually True? \n",
    "\n",
    "_recall (aka, sensitivity)_: \n",
    "- count(TP)/count(TP+FN). \n",
    "- How many of the True values do I recover in my predictions?\n",
    "\n",
    "_specificity_:\n",
    "- count(TN)/(TN+FP)\n",
    "- How many of the False values do I recover in my predictions?\n",
    "\n",
    "ROC: sensitivity vs specificity (True positive vs True negative)\n",
    "- use when trying to find best split\n",
    "- more common\n",
    "\n",
    "precision-recall:\n",
    "- use for \"needle-in-haystack\" positive examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### random forests\n",
    "[Random Forest](https://www.stat.berkeley.edu/~breiman/RandomForests/cc_home.htm)\n",
    "- group of decision trees, each vote and the result is the average\n",
    "- at the beginning, sample N data points with replacement (where N is total number of examples)\n",
    "   - The other samples can be held out (out-of-bag) and used to compute classification error, see below\n",
    "- only allow a subset of variables at each node of the tree\n",
    "- each tree is grown all the way to the bottom\n",
    "- do not overfit, because they are an ensemble\n",
    "- out-of-bag error: use the out-of-bag samples to compute a decision tree made up of only out-of-bag samples. The accuracy of all such trees is the out-of-bag error.\n",
    "- variable importance: take the trees made up of out-of-bag samples. Count the correct number of true predictions. Permute a single variable m across all trees, then count the number of true predictions. The difference here gives the feature importance\n",
    "- my question: how to get variable importance for a test data point propagated through the forest? What about subsetting for positive oob trees then averaging across these? Ugh, not sure...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "F1 score: harmonic mean of precision and recall\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### k means clustering [algorithm](http://stanford.edu/~cpiech/cs221/handouts/kmeans.html):\n",
    "\n",
    "1. randomly initialize cluster centers\n",
    "2. while error (sum of square euclidean distance) not decreasing:\n",
    "    - reassign labels to match the closest cluster centroid\n",
    "    - re-compute the cluster centroids based on the current labels\n",
    "    \n",
    "_Note_, curse of dimensionality in clustering. As dimensionality increases, all data appears closer and closer together,\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### regularization\n",
    "Regularization is needed to prevent overfitting. [this](http://openclassroom.stanford.edu/MainFolder/DocumentPage.php?course=MachineLearning&doc=exercises/ex5/ex5.html) is good.\n",
    "For example, linear model\n",
    "$$y = Ax$$\n",
    "then, if using L2 norm,\n",
    "$$J = (yTrue - A * x)^2 - lambda * A^2$$\n",
    "where lambda tunes the strength of the regularization.\n",
    "Alternatively, L1-norm forces individual terms to zero, can be used for variable selection. To find the ideal regularization strength, use cross validation.\n",
    "\n",
    "### cross-validation\n",
    "How does that cross validation work? For example, k–fold cross validation. Split the data into k folds. Train on len(k)-1, and use the last fold for evaluating model performance. Repeat such that each fold is the evaluation set exactly once. This allows you to mitigate bias (because all of the data is accounted for in the data), and also mitigates variance (because the performance is averaged across holdout sets, reducing the affect of non-representative samples in the holdout set).\n",
    "\n",
    "If you want to use cross validation on a tuning parameter (e.g., regularization), then use k–fold CV and pick the lowest average value (of R^2, or equivalent).\n",
    "\n",
    "### model selection\n",
    "If you want to use cross validation for testing between different models, then first, split into a training–test set. Within the training set, do k–fold cross validation to find the best parameter set for each model. After this, do the direct comparison on the test set. If you do not use the test set, then you are basing conclusions of model performance, where the model has explicitly been trained on the testing set.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### bias–variance\n",
    "What is the bias–variance tradeoff?\n",
    "Bias is when the model is not complex enough and cannot sufficiently explain the data. Variance is when a model is so overfit on the training data that it cannot be generalized to new data. The tradeoff is that if you increase complexity, you will eventually increase variance, but if you decrease the complexity, you can no longer accurately model the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### kNN question:\n",
    "What about if you have k-Nearest neighbors and a big feature space? How would this affect performance? I believe the interviewer said that many categorical values could slow down the computation time. So in _k_-means you have to 1) compute cluster centroids; and 2) update the cluster labels accordingly. have to compare pairwise distances, but I think that computing the distances. Think through this..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### how to test significance of predictors of purchasing propensity?\n",
    "First, there is the feature importance readouts in Random Forest/Logistic, but these don't give significant values. Second, you could try variable selection.\n",
    "\n",
    "Probably the best way is to use glm in R. This provides significance values per variable. [here](https://stats.stackexchange.com/questions/60817/significance-of-categorical-predictor-in-logistic-regression) explains how the significance values are calculated (Wald test). This is essentially the coef_estimate/std_error. The null is that the parameter value is zero. The p-value computes the the probability of getting these values, given that the null (paramter = 0) is true. Not quite sure why there are different std_dev for each variable. Maybe something related to [variance–covariance matrix](https://stats.stackexchange.com/questions/60074/wald-test-for-logistic-regression)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the weight values for a linear model\n",
    "1. Start with sum of squares error\n",
    "2. Take the derivative with respect to the weights\n",
    "3. Solve for W using matrix inversions and transpositions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Questions\n",
    "- What does PCA do?\n",
    "  - Finds axis of greatest variance. Then next PC is second highest variance & orthogonal. So you can express data in lower-dimensional space while maintaining most of the variance in the data. Also acts in a way of variable selection. If variable does not have a meaningful signal, it would be mainly in the low–variance PCs that are removed\n",
    "- Would PCA–transformed data perform better if running new data through clustering than data from hundred dimensional untransformed data?\n",
    "  - Yes. Curse of dimensionality is that as dimensionality increases, everything gets close together. So, since distances are more similar in high–dimensional space, random changes are more prone to corrupt the predicted labels\n",
    "- In what case would you care to use stratified vs. pure random splitting?\n",
    "  - Random sampling is fine if your sample sizes are very large, but if they are smaller, then there is a greater chance there is a significant change between randomly assigned predictors. So, when data size is smaller, need to have more advanced splitting strategies\n",
    "- Would you consider these four separate experiments? Would you compare the results across the clusters?\n",
    "  - I said I was only concerned about lift \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Demandbase post–mortem\n",
    "Seth Myers\n",
    "  - started with a bunch of ML questions, this was pretty easy for the most part. PCA, regularization, train/test split, etc.\n",
    "  - if you trained a model and you test and the accuracy is low, what could be wrong? How would you diagnose?\n",
    "  - how could you get individual \"feature importance\" for random forests?\n",
    "    - I suggested to permute the data labels and measure the change in accuracy. He liked the solution.\n",
    "    - I realized that I really need a good understanding of how a decision tree chooses to split on a variable. Like, actually draw out the graph\n",
    "  - Reservoir sampling question. I had no idea what was going on, but it was the end of the interview and he gave the solution\n",
    "  - Code naive Bayes in pyspark. I needed a slight correction\n",
    "\n",
    "Eric White\n",
    "  - asked about how to test for significance in A/B test for click through rate. Need to remember that the variance of binomial distribution is $p * (1-p)$. Otherwise it was good.\n",
    "  - asked about in which cases k–means would be a bad choice\n",
    "    - it assumes clusters are equally sized spheres. For example, what if variances are known to be very different.\n",
    "  - asked about how to determine the correct number of clusters to use for k-means\n",
    "    - look back at my old blog post...\n",
    "  - asked about my choice of permutation tests. Needed to explain that the expected value is zero of the sampling distribution.\n",
    "\n",
    "Vijay\n",
    "  - I thought this one went well. He asked a variety of how-to-set-up ML questions, I don't have a recollection of any time I felt stumped.\n",
    "  - One interesting conversation: say you have an ambiguous word, such as \"pipeline\", which could be used in oil industry, or in version control. I suggested keeping a running count of word occurences, or building a vector representation of words, then using similarity in high-dimenional space\n",
    "\n",
    "Derek\n",
    "  - I completely screwed up on reading the code for a recursive algorithm written for binary search. I couldn't get through a simple example and I got the worst–case time complexity wrong. I really need to work on my algorithms skill...\n",
    "- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
