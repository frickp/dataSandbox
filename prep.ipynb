{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_fibonacci(val):\n",
    "    if val == 0:\n",
    "        return 0\n",
    "    elif val == 1:\n",
    "        return 1\n",
    "    else:\n",
    "        return get_fibonacci(val - 1) + get_fibonacci(val - 2)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_fibonacci(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def factorial(n):\n",
    "    if n<=0:\n",
    "        return 1\n",
    "    else:\n",
    "        return n * factorial(n-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "factorial(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regularization imposes penalty on objective function. Thus\n",
    "\n",
    "For example, on a simple regression with one coefficient:\n",
    "equation:\n",
    "y - a_1*x + a_0\n",
    "\n",
    "Loss function:\n",
    "Linear regression\n",
    "\n",
    "$J = (y - \\hat{y})^2 + \\lambda A^TA$\n",
    "\n",
    "Lasso regression\n",
    "$J = p\\log(\\hat{p}) + (1-p)\\log(1 - \\hat{p}) + \\lambda A^TA$\n",
    "\n",
    "\n",
    "LASSO is a Laplacian prior and Ridge regression is a gaussian prior"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### My data science story\n",
    "Always sought to have quantitive understanding of the world. Physics degree. I wanted to learn something new, apply math/engineering to new domain (biology). Math modeling, data munging, statistical analysis, image processing. Working in relatively new domain (high-throughput imaging).\n",
    "\n",
    "In Stanford, I interviewed at 3 labs and chose to join the one with the best compute infrastructure. Audit ML course. Less drawn to grant writing, conducting experiments.\n",
    "\n",
    "AT&T\n",
    "\n",
    "Worked in center of excellence model in team of ~20 data scientists/engineers in bay area. Got exposure to large variety of algorithms and chance to work with vast amounts of real world data. Gave chance to grow in responsibility, to own interactions, to be technical lead, to mentor younger DS\n",
    "\n",
    "Quid\n",
    "\n",
    "Decided to change for cross functional component. Chance to work closely with both business stakeholders and engineers. Head of DS, VP Eng, CTO all convinced me of this story. 50/50 services/sales. Long term vision of SaaS company. Lot of service projects outside of scope of product. Need to expand the product coverage by converting data science processes into offered lightweights apps. In practice, constant asks and DS scripts were very scattershot. With engineering, I refactored and code and put it into a CI-integrated repository. From there, availability given to commercial team (consulting/customer success/sales) interactive webpage with voila, or Flask-hosted.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To do\n",
    "SQL zoo\n",
    "easy leetcode questions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear model assumptions\n",
    "- residuals are normally distributed and centered at zero\n",
    "- each data point is sampled independently (stationary). E.g., in MLE, $p(X|\\theta) = p(x1) * p(x2) * p(x3)...$\n",
    "- not heteroskedastic\n",
    "- Each feature is independent\n",
    "- Target is linear combination of features. One outcome of this is that it can be awkward to model a continuous feature, for example, if it is a normal distribution, where a record having the mean value should give the best gain, having a positive or negative weight would not capture this. A way around this would be to bucketize that feature (to one-hot-encode it, so that you can highlight certain parts of the distribution)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SQL\n",
    "- leetcode/hackerrank\n",
    "- SQL Zoo (facebook recruiter says this is best)\n",
    "- http://www.w3schools.com/sql (good, but not challenging)\n",
    "- db.grussell.org/sql\n",
    "- [this is pretty good](https://www.programmerinterview.com/database-sql/practice-interview-question-1-continued/)\n",
    "\n",
    "###  Suppose we have a log containing customer interactions on the Amazon website as shown below. Return a list of unique customers who visited the site every day over the past week.\n",
    "```\n",
    "SELECT user\n",
    "FROM table\n",
    "WHERE DATEDIFF(CURR(DATE), date) <= 7\n",
    "GROUP BY user, date\n",
    "HAVING COUNT(DISTINCT date) = 7\n",
    "```\n",
    "###  Sort items per user according to the most recent purchase activity\n",
    "```\n",
    "SELECT RANK() OVER(PARTITION BY user ORDER by date DESC)\n",
    "FROM table\n",
    "WHERE activity = \"purchase\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Binomial distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\binom{n}{k}\\quad p^k(1-p)^{(n-k)}$$\n",
    "Where \n",
    "$$\\binom{n}{k} = \\frac{n!}{k!(n-k)!}$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algos I know\n",
    "\n",
    "*PhD*\n",
    "- Linear model\n",
    "- t-test\n",
    "- KS-test\n",
    "- ordinary differential equations\n",
    "- developing analytics/models in new domain\n",
    "- correlation test\n",
    "\n",
    "*Stanford*\n",
    "- data processing on remote cluster\n",
    "    - python\n",
    "    - bash\n",
    "- project management\n",
    "    - intiate collaborations\n",
    "    - direct technician\n",
    "    - report findings in scientific meetings\n",
    "\n",
    "*AT&T*\n",
    "- PCA\n",
    "- resampling\n",
    "- random forest\n",
    "- neural network\n",
    "- tSNE\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Example analysis:\n",
    "- situation, task, action, result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quid\n",
    "### Situation\n",
    "When I decided to join Quid, vision from CTO was for utilizing new infrastructure for creating lots of new apps and expand our product offering. The reality at the ground level was much different. Many requests coming in to D.S. team from commercial team, lot of them were overlapping one off analyses. The modus operandi was to do services projects one at a time. Siloed interactions prevented knowledge sharing. \n",
    "\n",
    "### Task\n",
    "Collaborate with engineering organization to democratize data science to rest of org and within the DS team.\n",
    "\n",
    "### Action\n",
    "Build tested python library integrated into CI pipeline and drive adoption from rest of team. Host reproducible scripts on Flask-based website for access by non-technical users. Push private messages to applied data science slack channel.\n",
    "\n",
    "### Result\n",
    "Analyses that used to take a data scientist a half a day can now be done in less than day. Commercial team adopting new scripts hosted on Flask webpage. Core functionality being used in place to host jupyter notebooks via webpage (voila).\n",
    "\n",
    "## skills this displays\n",
    "- unpopular opinion. this was not what leadership saw as my role. team was happy with status quo. internal business clients kept expecting results fast.\n",
    "  - having backbone\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AT&T sentiment analysis\n",
    "### Situation\n",
    "In the first part of AT&T, we served as data science development shop, specializing in nlp. One of my favorite projects working there was related to developing a sentiment classifier that would work on Twitter, because we signed a trial with Twitter to ingest the whole English firehose.\n",
    "\n",
    "### Task\n",
    "Develop social media analysis tools to equip other teams\n",
    "\n",
    "### Action\n",
    "During my free time, I developed a sentiment classifier using deep learning libraries. Got buy in to do hyperparameter search on internal GPU server.\n",
    "\n",
    "### Result\n",
    "Outperformed current solution. Proved to be generalizable. Applied successfully to product releases and, when ranked by weighted sentiment, it recovered controversial product release covered by major news outlet. Invited to presented internally to ~100 people. On roadmap for microservices, before re-org happened.\n",
    "\n",
    "## what this shows\n",
    "initiative to learn. reaching out to get things done. innovative solution. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AT&T timeseries forecasting\n",
    "\n",
    "### situation\n",
    "Overall goal is to give Financial Office visibility into the free cash flow of the company, which is incredibly complex, due to a large number of revenue streams and billing systems. Without this visibility, if too liberal, can be caught needing to take out high interest debt and waste lots of money. If too conservative, that is money left on the table that could be invested. Business development team estimated value of project in the tens of millions range if we could outperform the baseline.\n",
    "\n",
    "Baseline model is the total receipts from 1 year ago. Can we do better using ML? Can we incorporate other features?\n",
    "\n",
    "### task\n",
    "Implement a time series forecast that outperforms the baseline models, especially at the end of quarter.\n",
    "\n",
    "### action\n",
    "I joined the team late. First phase was using prophet model for univariate forecasting. That is, look at the historical data to project forward. Evaluate using MSE. Challenge was how to incorporate other time series data, e.g., billing? If you incorporate it, should it have a delay? Is that different per payment type? My underlying hypothesis was to look for time series that correlated with the residuals of the univariate model. That is, the residuals are the *unexplained* variance. What factors may help to capture that variance. I therefore created a tool to quickly create a heatmap matrix of correlations. Each cell is the correlation coefficient. Each row is a different payment type. Each column is a range of date delays. Put into class to quickly instantiate and test.\n",
    "\n",
    "Other thing I did was to ease the collaboration on the team. Drive team-wide adoption of git branching. This helped junior DS keep up to date with the codebase and contribute key hypotheses to the rest of the team. Also, kind of spagetti code when I got there. Bloated functions, many variables to keep track of, and complicated logic. Abstracted to class. This allowed easy handoff to engineers. More interpretability. And easier to nucleate the efforts of all 3 DS on the team.\n",
    "\n",
    "### result\n",
    "Implemented and put into production. Found adding certain billing data significantly improved the \n",
    "\n",
    "This should be my go to project for a kind of classic ML approach. Actually was deployed using Jenkins. Displayed some software/leadership skills and can have the classic eval metrics.\n",
    "\n",
    "If they ask about prophet model, it is general additive model. Assumes that target is the sum of each component feature, which does not have linearity constraint. So you can easily model forecast = trend + residuals + yearly seasonality + weekly seasonality. Grid search to find the optimal coefficients. For example, how much to regularize the seasonality, etc? Way to evaluate is a rolling window. So get all the 1 day forecasts, all the 2 day forecasts, etc and aggregate them. You can then compute the mean and the standard error. From this you can compute confidence intervals to say if it significantly outperformed another model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### clustering zip codes for \n",
    "From joined data (third party- and internal), find zip codes to target. First, feature engineering. Bucket data. Transform using PCA to keep only salient variance. Compute distances (I think it was cosine similarity). Do tSNE to map to low dimensional space. Do DB-scan. Examine the features most over-under represented in features. Display zip codes on map out zip codes to check quality of clusters. \n",
    "\n",
    "How did I get higher-than-expected values? Keep data indices, so can cluster using principal components, but interpret by using the normalized variables. Variables were standardized independent of clusters. Then, use the cluster labels as a mask and compute the mean value for each variable. Then sort by the absolute value to find best variables.\n",
    "\n",
    "Do surveys to determine effectiveness of campaign. After the fact, did chi-squared test to find targetable variables. How did this work? People gave propensity 1–10, map this to suspeptible (4–7) or not (Thus, categorical variables). Then do a chi-squared test to test for independence. Null hypothesis is that there is no significant difference between susceptible/non–susceptible determine if the variable was significantly different. Also, power analysis to check the number of surveys needed to achieve a significant result.\n",
    "\n",
    "Outcome: achieved success defined by our business client. Got bonus and recognition for work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### VAR for potential interactions and combining knowledge\n",
    "Premise was that AT&T paid big money to buy several different analyst reports on market forecasts. Once a year CEO and corporate strategy team would review them during a planning session. Problem is that there is no information transfer between the forecasts. Or cannot use other economic indicators. Team was very interested in feedback between data points or early indicators. This was the first interaction of the strategy team with DS/stats people. \n",
    "\n",
    "When they brought the problem to me, I saw several challenges and explained the limitations as clearly as I could.\n",
    "For one, it was quarterly data, and they wanted to project years into future (predict years forward given 20 datapoints or historical info). Also, this was in a regime in which p > n, so not able to get feature importances in way you would using a classic ML approach. Also, this rests on the assumption that the world's future behavior follows the rules of the past. \n",
    "\n",
    "They still wanted to move forward and present an \"internal view\" forecast along with the other forecasts. What I proposed as useful was, rather than focusing on exact values for an evaluation metric, I could provide uncertainty estimates and look for between-variable interactions.\n",
    "\n",
    "I decided to use vector autoregression (VAR). This technique forecasts several variables together. Also, this technique lends itself to test for interactions. I found that I could test a given variable along with 2 other variables, and test for significant interactions, using [Granger causality](http://www.scholarpedia.org/article/Granger_causality). Essentially, this will have a null hypothesis that the interaction term is zero and low p-value rejects the null, saying the interaction is real. Another way of saying that is that the explained variance is significantly increased if adding the additional variable.\n",
    "\n",
    "So for the forecasts of interest (f), I chose n-choose-2 of the remaining time series features to forecast together. Then I kept the results of all forecasts. Additionally, I recorded forecasts where the p-value was below 0.05 for Granger causality test. So what I presented was 1) shaded lines of all forecasts; 2) ordered list of variables that wre significant (along with the directionality of that interaction).\n",
    "\n",
    "y1 = A1x1(t-j, t-1) + B1x2(t-j, t-1)\n",
    "y2 = A2x2(t-j, t-1) + B2x1(t-j, t-1)\n",
    "\n",
    "In above, top right Granger causality tests if top right term significantly non-zero."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Quid time series dynamics.\n",
    "Quid roughly 50/50 software sales and services sales. The vision for bringing me on was to translate the money from the service side to software via reusable capabilities and products drawn from direct asks from high value clients. So 1) deliver on service contracts; 2) extract reusable data science capabilities for downstream work. Business value is the actual $, enabling new capabilities\n",
    "\n",
    "As a quick intro, Quid provides insights from unsupervised clustering applied to textual data. User inputs a boolean query, then the application retrieves articles according to a relevence score (ES), and output is thousands of articles (for visualization engine). Then these articles are passed into an unsupervised output is clusters, dynamics, influencers, etc. To do this, the application searches an index of billions of articles, and is reduced to thousands of articles after a relevancy criteria.\n",
    "\n",
    "However, some questions come up all the time in sales/consulting calls. 1) can I rely on this as an encompassing view if it only accounts for a couple thousand queries; 2) how can I compare across queries when I do not need to rely on unsupervised clustering. That is, if I would rather have a broader view than I came with, rather than a fine grained view of everything within a query. 3) How does a trend change over time?\n",
    "\n",
    "To deal with this I built a python library to interact directly with ElasticSearch. Collaborated with Search team to know limits, gathered disparate code from other data scientists, and worked with engineering to get it integrated into our codebase, integrated with Travis CI and available from our internal pypi repository. I implemented functions for counting and direct query and worked with Infra team to host these via Binderhub and Voila.\n",
    "\n",
    "An example capability was to see how a trend changed over time. In this case, nested aggregation of metadata (by country, then by date) allows an analyst to see, for example, how the news share for a certain topic changes with respect to both time and geography.\n",
    "\n",
    "Getting time series metrics also allows correlation analysis. This allows one to see between-trend connections where they weren't explicitly tested for. So for example, a corporate strategy team may be interested in trends such as climate, innovation, equality, scandals, etc., if you define each of these, do a pairwise correlation to surface connections between trends."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Product intuition\n",
    "This is a relatively weak area for me. For the instacart interview, I was asked, \"should we implement surge pricing\"? It was clear I was out of my comfort zone. I said it could be bad for customers, since they may not like it, and bad for instacart for lost revenue for those underbooked hours. I tried coming up with a metric about the rate of dropped carts, to capture customers who do not end up checking out because they are frustrated by the high prices. And I suggested an A/B test, but I could not think through whether to split by geographic region or not. There were some other questions, but I was flustered and don't remember everything.\n",
    "\n",
    "If I have a stab at it now, I would say that the incentives align to try it. For the customers, it is the difference between not being able to do anything and paying a premium for something that is providing value for you. For the shoppers, it is a chance to earn extra money. For Instacart, it is the opportunity to increase satisfaction since there are network effects and business is growing.\n",
    "\n",
    "From a business perspective, I'd like to know if people are exposed to surge pricing have a higher or lower satisfaction. For that I'd like to give out surveys to people who are either exposed to search pricing or not. Do people who end up buying surge pricing have higher satisfaction? One way that you could do this is to do an A/B test. Randomizing would not work, want to capture natural demand. Would do a 1 tailed t-test. If they ask about whether to do this per market, I'd say to do stratified sampling. Meaning that a sampling strategy such that the relative size of markets is balanced across A group and B group.\n",
    "\n",
    "I'd also like to track the rate of open slots. Ideally, that should be less than full. One way you could do that is to try to model this as a binomial process. Tune the supply-demand model such that the expected value is two standard errors less than 1.0.\n",
    "\n",
    "For A/B test, only change 1 thing at a time. If you suspect these 2 are highly entangled, you could do A/B/C test and do some multiple hypothesis correction. But in general it is best to do one thing at a time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LinkedIn\n",
    "\n",
    "Recruiter gave me a hypothetical question about \"should we lower the number of free Inmails per month\"? From a business perspective, I would imagine that the funnel would be to 1) get consistent users of InMails (recruiters, job seekers); 2) once they like it, convert to high value account.\n",
    "\n",
    "Lowering the number of InMails may \n",
    "\n",
    "First off, I'd like to understand who are the people that may be at risk for churn, or that may be more susceptible to be convertible to a high-value customer. \n",
    "\n",
    "In the case of 1) want to look at MAU for the Inmail product."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### merge sort/bubble sort\n",
    "sorted list from two sorted lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 1, 2, 5, 6, 9]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bubble_sort([9,1,5,2,6,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sorted_union(srt_1, srt_2):\n",
    "    result = [0] * (len(srt_1)+len(srt_2))\n",
    "    last_1 = len(srt_1) - 1\n",
    "    last_2 = len(srt_2) - 1\n",
    "    i, j, k = 0, 0, 0\n",
    "    while (i < last_1) & (j < last_2):\n",
    "        if srt_1[i] < srt_2[j]:\n",
    "            result[k] = srt_1[i]\n",
    "            i += 1\n",
    "            k += 1\n",
    "        else:\n",
    "            result[k] = srt_2[j]\n",
    "            j += 1\n",
    "            k += 1\n",
    "    while i <= last_1:\n",
    "        result[k] = srt_1[i]\n",
    "        k += 1\n",
    "        i += 1\n",
    "    while j <= last_2:\n",
    "        result[k] = srt_2[j]\n",
    "        k += 1\n",
    "        j += 1\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 3, 4, 5, 7, 6, 8]"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted_union([1,3,5,7],[4,6,8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 2, 3, 3, 3, 4, 4, 5]"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted_union([1,2,3,3],[2,3,4,4,5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A/B testing? Binomial significance test?\n",
    "\n",
    "[This](https://www.lunametrics.com/blog/2014/07/01/statistical-significance-test/) argues for using chi-squared test for A/B testing. Seems alright, but it is a _rate_, not counts that are important. This would be helpful for Probably Z-test in quora answer is best. [This](http://soc.utah.edu/sociology3112/chi-square.php) is a nice explanation on chi-squared tests. Null is that there is no relationship the variables\n",
    "\n",
    "[This](https://www.quora.com/What-statistical-test-should-be-used-for-an-A-B-test-on-conversion-rates#) looks pretty good.\n",
    "\n",
    "Let me try to summarize on-the-fly. Say you want to test a different layout on your website. The way you would test this would be to run an A/B test. First, users would be randomized to either control or treatment. The metric to quantify would be ctr (clickthrough rate). Either a user will click or not (0 or 1 per user), and this can be modeled by the binomial distribution, where the expected value is $$p(click)/(p(click)+p(no_click))$$ First, we define the null hypothesis, which is that $$ctrTreatment <= ctrControl$$. Or alternatively, $$ctrTreatment - ctrControl <= 0$$ We set a significance threshold of p = 0.05. In this case, I choose a one-tailed test. The distribution of proportions will be binomially distributed, but since we have a large number of observations, we can assume that the distribution of ctrTreatment - ctrControl is normal (because of the central limit theorem). Therefore, we use a Z-test. Calculate the z-statistic. Z = (mean_obs - mean_null)/(stdev/sqrt(n)) = (mean_obs/std_error). Then check if Z > 1.65. If so, this is significant at 95% confidence level.\n",
    "\n",
    "To compute the standard error, we will use stdev/sqrt(n). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### re-watch the kahn academy significance testing videos.\n",
    "[This one](https://www.khanacademy.org/math/statistics-probability/significance-tests-one-sample/tests-about-population-proportion/v/large-sample-proportion-hypothesis-testing) on testing proportions is good."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### T-Test\n",
    "p–value: probability of getting results this extreme or more given that the null hypothesis is true\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://medium.com/airbnb-engineering/experiments-at-airbnb-e2db3abf39e7\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### classification eval metrics\n",
    "_precision_:\n",
    "- TP/(TP + FP)\n",
    "- I predicted some values as True. What proportion of these predictions are actually True? \n",
    "\n",
    "_recall (aka, sensitivity)_: \n",
    "- count(TP)/count(TP+FN). \n",
    "- How many of the True values do I recover in my predictions?\n",
    "\n",
    "_specificity_:\n",
    "- count(TN)/(TN+FP)\n",
    "- How many of the False values do I recover in my predictions?\n",
    "\n",
    "ROC: sensitivity vs specificity (True positive vs True negative)\n",
    "- use when trying to find best split\n",
    "- more common\n",
    "\n",
    "precision-recall:\n",
    "- use for \"needle-in-haystack\" positive examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### random forests\n",
    "[Random Forest](https://www.stat.berkeley.edu/~breiman/RandomForests/cc_home.htm)\n",
    "- group of decision trees, each vote and the result is the average\n",
    "- at the beginning, sample N data points with replacement (where N is total number of examples)\n",
    "   - The other samples can be held out (out-of-bag) and used to compute classification error, see below. This should not replace a test set, since then the data will have been seen in some training instances.\n",
    "- only allow a subset of variables at each node of the tree\n",
    "- each tree is grown all the way to the bottom\n",
    "- do not overfit, because they are an ensemble\n",
    "- out-of-bag error: use the out-of-bag samples to compute a decision tree made up of only out-of-bag samples. The accuracy of all such trees is the out-of-bag error.\n",
    "- variable importance: take the trees made up of out-of-bag samples. Count the correct number of true predictions. Permute a single variable $m$ across all trees, then count the number of true predictions. The difference here gives the feature importance\n",
    "- my question: how to get variable importance for a test data point propagated through the forest? What about subsetting for positive oob trees then averaging across these? Ugh, not sure...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "F1 score: harmonic mean of precision and recall\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### k means clustering [algorithm](http://stanford.edu/~cpiech/cs221/handouts/kmeans.html):\n",
    "\n",
    "1. randomly initialize cluster centers\n",
    "2. while error (sum of square euclidean distance) not decreasing:\n",
    "    - reassign labels to match the closest cluster centroid\n",
    "    - re-compute the cluster centroids based on the current labels\n",
    "    \n",
    "_Note_, curse of dimensionality in clustering. As dimensionality increases, all data appears closer and closer together,\n",
    "\n",
    "The formal objective in k-means is to minimize the summed variance of all clusters. Where the variance is sum((x-mu)^2) for each point in cluster.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### regularization\n",
    "Regularization is needed to prevent overfitting. [this](http://openclassroom.stanford.edu/MainFolder/DocumentPage.php?course=MachineLearning&doc=exercises/ex5/ex5.html) is good.\n",
    "For example, linear model\n",
    "$$y = Ax$$\n",
    "\n",
    "where x is $(n, m)$ matrix, and $A$ is a $(m, 1)$ vector, and $y$ is $(n, 1)$ matrix\n",
    "\n",
    "then, if using L2 norm,\n",
    "$$J = (y - Ax)^2 - \\lambda A^TA$$\n",
    "where lambda tunes the strength of the regularization.\n",
    "Alternatively, L1-norm forces individual terms to zero, can be used for variable selection. To find the ideal regularization strength, use cross validation.\n",
    "\n",
    "### cross-validation\n",
    "How does that cross validation work? For example, k–fold cross validation. Split the data into k folds. Train on len(k)-1, and use the last fold for evaluating model performance. Repeat such that each fold is the evaluation set exactly once. This allows you to mitigate bias (because all of the data is accounted for in the training), and also mitigates variance (because the performance is averaged across holdout sets, reducing the affect of non-representative samples in the holdout set).\n",
    "\n",
    "If you want to use cross validation on a tuning parameter (e.g., regularization), then use k–fold CV and pick the lowest average value (of R^2, or equivalent).\n",
    "\n",
    "### model selection\n",
    "If you want to use cross validation for testing between different models, then first, split into a training–test set. Within the training set, do k–fold cross validation to find the best parameter set for each model. After this, do the direct comparison on the test set. If you do not use the test set, then you are basing conclusions of model performance, where the model has explicitly been trained on the testing set.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### bias–variance\n",
    "What is the bias–variance tradeoff?\n",
    "Bias is when the model is not complex enough and cannot sufficiently explain the data. Variance is when a model is so overfit on the training data that it cannot be generalized to new data. The tradeoff is that if you increase complexity, you will eventually increase variance, but if you decrease the complexity, you can no longer accurately model the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### k-means question:\n",
    "What about if you have k-means and a big feature space? How would this affect performance? I believe the interviewer (Intuit) said that many categorical values could slow down the computation time. So in _k_-means you have to 1) compute cluster centroids; and 2) update the cluster labels accordingly. have to compare pairwise distances, but I think that computing the distances. Think through this..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### how to test significance of predictors of purchasing propensity?\n",
    "First, there is the feature importance readouts in Random Forest/Logistic, but these don't give significant values. Second, you could try variable selection.\n",
    "\n",
    "You could test for practical significance. If you want to test for practical significance, look at the performance metric, then ablate the feature of interest and retrain on train set. Does the ablated model performance substantially worse on the test set??\n",
    "\n",
    "Or you could test for statistical significance. I came up with some stuff below, which is not too convincing to me. I would sample the training data with replacement and fresh parameter estimates each time. Then I would see if mean + 2 st. dev is greater either completely above or below zero.\n",
    "\n",
    "_OLD_:\n",
    "Probably the best way is to use glm in R. This provides significance values per variable. [here](https://stats.stackexchange.com/questions/60817/significance-of-categorical-predictor-in-logistic-regression) explains how the significance values are calculated (Wald test). This is essentially the coef_estimate/std_error. The null is that the parameter value is zero. The p-value computes the the probability of getting these values, given that the null (paramter = 0) is true. Not quite sure why there are different std_dev for each variable. Maybe something related to [variance–covariance matrix](https://stats.stackexchange.com/questions/60074/wald-test-for-logistic-regression)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Demandbase post–mortem\n",
    "Seth Myers\n",
    "  - started with a bunch of ML questions, this was pretty easy for the most part. PCA, regularization, train/test split, etc.\n",
    "  - if you trained a model and you test and the accuracy is low, what could be wrong? How would you diagnose?\n",
    "  - how could you get individual \"feature importance\" for random forests?\n",
    "    - I suggested to permute the data labels and measure the change in accuracy. He liked the solution.\n",
    "    - I realized that I really need a good understanding of how a decision tree chooses to split on a variable. Like, actually draw out the graph\n",
    "  - Reservoir sampling question. I had no idea what was going on, but it was the end of the interview and he gave the solution\n",
    "  - Code naive Bayes in pyspark. I needed a slight correction\n",
    "\n",
    "Eric White\n",
    "  - asked about how to test for significance in A/B test for click through rate. Need to remember that the variance of binomial distribution is $p * (1-p)$. Otherwise it was good.\n",
    "  - asked about in which cases k–means would be a bad choice\n",
    "    - it assumes clusters are equally sized spheres. For example, what if variances are known to be very different.\n",
    "  - asked about how to determine the correct number of clusters to use for k-means\n",
    "    - look back at my old blog post...\n",
    "  - asked about my choice of permutation tests. Needed to explain that the expected value is zero of the sampling distribution.\n",
    "\n",
    "Vijay\n",
    "  - I thought this one went well. He asked a variety of how-to-set-up ML questions, I don't have a recollection of any time I felt stumped.\n",
    "  - One interesting conversation: say you have an ambiguous word, such as \"pipeline\", which could be used in oil industry, or in version control. I suggested keeping a running count of word occurences, or building a vector representation of words, then using similarity in high-dimenional space\n",
    "\n",
    "Derek\n",
    "  - I completely screwed up on reading the code for a recursive algorithm written for binary search. I couldn't get through a simple example and I got the worst–case time complexity wrong. I really need to work on my algorithms skill...\n",
    "- "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Stitch fix algos\n",
    "algos they used in tour:\n",
    "- collaborative filtering\n",
    "- mixed effects model\n",
    "- neural networks to learn style embeddings\n",
    "- nlp for what customers want (lda2vec)\n",
    "\n",
    "I should look at...\n",
    "- MLE understanding of linear models\n",
    "- look at linear regression wikipedia\n",
    "- given a customer who just signed up, how to suggest potential items?\n",
    "- svd like netflix challenge?\n",
    "- read over LDA stuff again\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('a', 3), ('c', 8), ('b', 2), ('d', 4)])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# How to join dictionaries with different keys?\n",
    "\n",
    "a = {\"a\": 1, \"b\": 2, \"c\": 3}\n",
    "b = {\"a\": 2, \"c\": 5, \"d\": 4}\n",
    "\n",
    "def join_dicts(a, b):\n",
    "    \"\"\"Join two dictionaries.\n",
    "    \n",
    "    a (dict):\n",
    "        int-valued dictionary\n",
    "    \n",
    "    b (dict):\n",
    "        int-valued dictionary\n",
    "    \n",
    "    \"\"\"\n",
    "    c = OrderedDict()\n",
    "    for key in set(a.keys()).union(b):\n",
    "        c[key] = a.get(key, 0) + b.get(key, 0)\n",
    "    return c\n",
    "\n",
    "join_dicts(a, b)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML metrics\n",
    "\n",
    "### precision:\n",
    "\"for predictions I make, what is the probability they are right?\"\n",
    "p(True | pred_true)\n",
    "TP / (TP + FP)\n",
    "\n",
    "\n",
    "### recall:\n",
    "\"What fraction of True cases do I recover?\"\n",
    "p(pred_true | True)\n",
    "TP / (TP + FN)\n",
    "\n",
    "### sensitivity:\n",
    "\"Of all the True events, how good am I at detecting them? E.g., how sensitive is my model?\"\n",
    "TP / TP + FN\n",
    "\n",
    "### specificity:\n",
    "\"How specific is my model? E.g., can I rightfully remove False values?\"\n",
    "TN / TN + FP\n",
    "\n",
    "    \n",
    "### ROC\n",
    "y: True positive rate\n",
    "x: False positive rate, or 1 - specificity\n",
    "    \n",
    "Want to get higher AUC on curve. Says you can get more True positives without the false negatives.\n",
    "\n",
    "### cross validation, hold out set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# odds ratio think through it\n",
    "# MLE think through it for linear model\n",
    "# biased coin toss problem\n",
    "# Speed up sql code?\n",
    "# think over categorical cross entropy (esp for logistic regression)\n",
    "# random forest look over derivation, loss function, draw the trees\n",
    "# write out equations for PCA\n",
    "# think through naive bayes. calculate by hand.\n",
    "# think through coding naive bayes\n",
    "# how to deal with imbalanced data\n",
    "# talk through k-means. derive it on paper.\n",
    "# find some DS questions on \n",
    "# do a bubble sort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code binary search\n",
    "arr = [1, 2, 3, 4, 7, 9]\n",
    "def binary_search(arr, target):\n",
    "    \"\"\"Given a sorted array of integers, check if `target` is contained in it.\n",
    "    \n",
    "    Test case:\n",
    "    arr = [1, 2, 3, 4, 7, 9]\n",
    "    \n",
    "    # this one\n",
    "    target = 2\n",
    "    ix = 3\n",
    "    curr = 4\n",
    "    arr = [1, 2, 3]\n",
    "    ix = 3 // 2 # 1\n",
    "    curr arr[ix] #2\n",
    "    \n",
    "    \n",
    "    target = 8\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    if target > len(arr):\n",
    "        return False\n",
    "    \n",
    "    if target < arr[0]:\n",
    "        return False\n",
    "    \n",
    "    if len(arr) == 1 and arr[0] != target:\n",
    "        return False\n",
    "\n",
    "    ix = len(arr) // 2\n",
    "\n",
    "    if target == curr:\n",
    "        return True\n",
    "    \n",
    "    if target < curr:\n",
    "        return binary_search(arr[0:ix], target)\n",
    "    \n",
    "    if target > curr:\n",
    "        return binary_search(arr[ix + 1:], target)\n",
    "    \n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "binary_search(arr, 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_lists(a, b):\n",
    "    \"\"\"Merge two sorted lists.\n",
    "    \n",
    "    You can assume that length is greater than 1, that only `int` in it.\n",
    "    \n",
    "    Test case\n",
    "    a = [1, 1, 2, 4, 5]\n",
    "    b = [2, 3, 5]\n",
    "    \n",
    "    out = [1, 1, 2, 2, 3, 4, 5, 5]\n",
    "    \n",
    "    \"\"\"\n",
    "    a_len = len(a)\n",
    "    b_len = len(b)\n",
    "    \n",
    "    out = []\n",
    "    \n",
    "    # ix_out = 0\n",
    "    ix_a, ix_b = 0, 0\n",
    "    \n",
    "    while ix_a < a_len and ix_b < b_len:\n",
    "        if a[ix_a] <= b[ix_b]:\n",
    "            out.append(a[ix_a])\n",
    "            ix_a += 1\n",
    "            continue\n",
    "        out.append(b[ix_b])\n",
    "        ix_b += 1\n",
    "    \n",
    "    while ix_a < a_len:\n",
    "        out.append(a[ix_a])\n",
    "        ix_a += 1\n",
    "    \n",
    "    while ix_b < b_len:\n",
    "        out.append(b[ix_b])\n",
    "        ix_b += 1\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 3, 3, 3, 4, 6, 6, 7, 8, 8, 9, 10, 11, 11, 11, 30]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merge_lists([1, 4, 6, 7, 8, 9, 10], [3, 3, 3, 6, 8, 11, 11, 11, 30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bubble_sort(arr):\n",
    "    \"\"\"Sort an array using bubble sort.\n",
    "    \n",
    "    Args:\n",
    "        arr (list):\n",
    "            Array of integers\n",
    "            \n",
    "    Returns:\n",
    "        list: sorted array\n",
    "        \n",
    "        \n",
    "    [5, 3, 4, 2, 1]\n",
    "    \n",
    "    first check positions 0, 1\n",
    "    5 < 3, so swap them\n",
    "    \n",
    "    temp = right\n",
    "    right = left\n",
    "    left = temp\n",
    "    \n",
    "    then check positions 1, 2\n",
    "    5 > 4, so swap them\n",
    "    \n",
    "\n",
    "    \"\"\"\n",
    "    for _ in range(len(arr)):\n",
    "        for i in range(len(arr) - 1):\n",
    "            if arr[i] > arr[i + 1]:\n",
    "                temp = arr[i + 1]\n",
    "                arr[i + 1] = arr[i]\n",
    "                arr[i] = temp\n",
    "    return arr\n",
    "                \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 1, 2, 5, 5, 7, 9]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bubble_sort([5,2,5,1,1,9,7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def compute_gini_for_single_node(labels):\n",
    "    \"\"\"Compute Gini Impurity given a set of labels.\n",
    "    \n",
    "    Gini impurity quantifies the chance of getting the wrong label if assigned at random.\n",
    "    \n",
    "    Intuition: When finding a feature to split on, you want to have a \"pure\" split, so\n",
    "    the goal is to minimize the gini impurity in each of the nodes downstream. A better\n",
    "    split has two child nodes that are more \"pure\" (less impure), meaning you effectively split,\n",
    "    for example, [0, 1, 0, 0] to one node, and [1, 1, 1, 1, 0] to the other node.\n",
    "    \n",
    "    \n",
    "    Args:\n",
    "        labels (iterable):\n",
    "            prediction labels downstream of classifier\n",
    "            \n",
    "    Returns:\n",
    "        float: gini impurity\n",
    "    \"\"\"    \n",
    "    label_counter = defaultdict(int)\n",
    "    label_len = len(labels)\n",
    "    \n",
    "    for label in labels:\n",
    "        label_counter[label] += 1\n",
    "    \n",
    "    gini_impurity = 1\n",
    "    \n",
    "    for label, label_ct in label_counter.items():\n",
    "        label_prob = label_ct / label_len\n",
    "        gini_impurity -= label_prob ** 2\n",
    "        \n",
    "    return gini_impurity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2777777777777777"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_gini([1,1,1,0,0, 0])\n",
    "compute_gini([0,0,0,0,1, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_split_gini(node_1_labels, node_2_labels):\n",
    "    gini_1 = compute_gini_for_single_node(node_1_labels)\n",
    "    gini_2 = compute_gini_for_single_node(node_2_labels)\n",
    "    len_1 = len(node_1_labels)\n",
    "    len_2 = len(node_2_labels)\n",
    "    weighted_gini = len_1 / (len_1 + len_2) * gini_1 + len_2 / (len_1 + len_2) * gini_2\n",
    "    return weighted_gini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.34444444444444433"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_split_gini([0, 0, 0, 1], [1, 1, 1, 0, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4888888888888889"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_split_gini([0, 0, 1, 1], [1, 0, 1, 0, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decision_tree_pseudocode():\n",
    "    \"\"\"Think through how a decision tree finds the best feature.\n",
    "    \"\"\"\n",
    "    impurity_dict = {}\n",
    "    for feat in features:\n",
    "        feat_values = get_feat_candidates()\n",
    "        for feat_value in feat_values:\n",
    "            pos_pred, neg_pred = threshold_data(arr, thresh_val)\n",
    "            pos_gini = compute_gini(pos_pred)\n",
    "            neg_gini = compute_gini(neg_pred)\n",
    "            len_pos, len_neg = len(pos_pred), len(neg_pred)\n",
    "            weighted_gini = len_pos / (len_pos + len_neg) * pos_gini + len_neg / (len_pos + len_neg)\n",
    "            impurity_dict[weighted_gini] = (feat, feat_value)\n",
    "\n",
    "    best_gini_vals = min(impurity_dict.keys())\n",
    "    best_feature, best_split = impurity_dict[best_gini_vals]\n",
    "    return best_feature, best_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feat_candidates(feat, ):\n",
    "    \"\"\"\n",
    "    If categorical, try each of the feature values\n",
    "    \n",
    "    If numeric, use the feature values themselves to split.\n",
    "    \n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dot_product(vec_a, vec_b):\n",
    "    \"\"\"Compute dot product of 2 vectors.\n",
    "    \n",
    "    Where dot product of two j-dimensional vectors, (a, b), is defined as\n",
    "        a_1 * b_1 + a_2 * b_2 ... a_j, b_j\n",
    "    \"\"\"\n",
    "    \n",
    "    assert len(vec_a) == len(vec_b)\n",
    "    mult_by_dim = [vec_a[ix] * vec_b[ix] for ix in range(len(vec_a))]\n",
    "    return sum(mult_by_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### given login and logout times, when are the most users logged in?\n",
    "```\n",
    "login logout\n",
    "2 5\n",
    "3 7\n",
    "4 5\n",
    "1 9\n",
    "\n",
    "SELECT a.time, COUNT(*)\n",
    "FROM table t\n",
    "JOIN\n",
    "    (SELECT login AS time\n",
    "    FROM\n",
    "    table\n",
    "    UNION\n",
    "    SELECT logout AS time\n",
    "    FROM\n",
    "    table) a\n",
    "on a.time\n",
    "BETWEEN t.login AND t.logout\n",
    "GROUP BY a.time\n",
    "ORDER BY COUNT(*) DESC\n",
    "LIMIT 1\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ LTV = avg\\ purchase\\ per\\ order * avg\\ orders\\ per\\ yr * avg\\ yrs\\ customer $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$Cust\\ acq\\ cost = total\\ acq\\ spend / number\\ new\\ customers$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# how to determine which clustering is better?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ TFIDF = \\frac{term\\ freq\\ in doc}{log(doc\\ freq)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### amazon phone screen (1/30/2020)\n",
    "SQL: If you can do group by count, and where, its really that simple.\n",
    "    \n",
    "Given a table of `ratings`, `user_id`, `movie_id`, and `date` how would you go about exploring it?\n",
    "- I mentioned doing a simple histogram and checking the potential for little signal if most are 5 stars. I also discussed time series comparisons on log scale to see movies that are growing in popularity. Also had a discussion of representing this data in a lower dimensional space by SVD/PCA and checking if variance explained is high enough to move forward. Had a very good discussion there, but I should have also said to check:\n",
    "1) the average rating of each movie;\n",
    "- are there movies that are good to recommend to all?\n",
    "2) standard deviation of each movie;\n",
    "- then you could just do a dumb z-score and have a very coarse, quick recommender\n",
    "3) Are there movies that have high ratings but low numbers? These are things I would like to examine how to prioritize in a recommender\n",
    "4) What about missing data?\n",
    "- One other things that came up was – if you have 2 clusterings from different techniques, which would you say is better. I mentioned NMI, but can't recall it off top of mind, but also consider [Silhouette score](https://en.wikipedia.org/wiki/Silhouette_(clustering)). From wiki: \"The silhouette value is a measure of how similar an object is to its own cluster (cohesion) compared to other clusters (separation)\"\n",
    "\n",
    "\n",
    "If you wanted to find product reviews where it had safety concerns, how would you do that (build classifier, imbalanced class)? What if you had no labels (manual labeling, but it is expensive, so build out a custom ontology and get a bigger set of words. Then try clutering or a topic model to get rough model. Run a classifier trained on that dumb model to get a set to send to labelers. Given raters with 3 different labels what would you do? Get a truth set first of high confidence labels, then compare that to 1-, 2-, or 3-rater agreement segments. You can compare metrics like recall across each segment to see to what degree they are a proxy for truth. Lastly, build a baseline classifier, like logistic regression, with test set, and compare more sophisticated models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.choice([0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 196 µs, sys: 1e+03 ns, total: 197 µs\n",
      "Wall time: 205 µs\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time all(np.random.choice([0,1]) for _ in range(100_000_000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1024"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2 ** 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.choice([0,1], p=[0.1, 0.9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "def coin_flip(bias):\n",
    "    if bias:\n",
    "        return 1\n",
    "    else:\n",
    "        return np.random.choice([0, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_coin():\n",
    "    return np.random.choice([0, 0, 0, 0, 0, 0, 0, 0, 0, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "deque([6, 4, 0, 3, 7])"
      ]
     },
     "execution_count": 270,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import deque\n",
    "d = deque()\n",
    "d.appendleft(6)\n",
    "d.append(4)\n",
    "d.append(0)\n",
    "d.append(3)\n",
    "d.append(7)\n",
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "#list(d)[-3:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "deque([6, 0, 4, 4])"
      ]
     },
     "execution_count": 255,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 258,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[6, 0, 4, 4]"
      ]
     },
     "execution_count": 263,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- explain neural network\n",
    "- Write a code to switch a k-th element with (N-k)-th element in a linked list of length N\n",
    "- Explain: EM algorithm \n",
    "- Coding: search a 2D matrix\n",
    "- scale before PCA?\n",
    "- How to find three element from 3 arrays that have sum of 0\n",
    "- think through gradient descent\n",
    "- write out formula for regularization\n",
    "- vanishing gradient? explain lstm\n",
    "- What are the assumptions for using a logistic regression?\n",
    "  - What is the loss function for it?\n",
    "- One coding question based on DFS\n",
    "- Tree level order traversal\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# python indexing\n",
    "consider a list `l = [5, 2, 6, 1, 8, 3, 6]`\n",
    "\n",
    "the last element: `l[-1]`\n",
    "\n",
    "everything except the last element: `l[:-1]`\n",
    "\n",
    "the `i`th element: `l[i+1]`\n",
    "\n",
    "three elements, starting from `i`: `l[i-1:i+2]`\n",
    "\n",
    "- `i-1`: because python is 0-indexed\n",
    "- `i+2`: `i-1` + `3`\n",
    "\n",
    "take a list and iterate through indices: `range(len(l))`\n",
    "\n",
    "range wants an int! I forget this a lot and sometimes do `range(l)`, WRONG!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def check_sum_consec(l, target):\n",
    "    \"\"\"Given a list of non-negative `int`, count how many consecutive\n",
    "    sublists that add to target.\n",
    "    \n",
    "    >>> l = [5, 2, 6, 1, 8, 3, 6]\n",
    "    >>> check_sum_consec(l, 9)\n",
    "    2\n",
    "    \"\"\"\n",
    "    count = 0\n",
    "    for ix in range(len(l)):\n",
    "        curr_sum = 0\n",
    "        while curr_sum < target and ix + 1 <= len(l):\n",
    "            curr_sum += l[ix]\n",
    "            if curr_sum == target:\n",
    "                count += 1\n",
    "            ix += 1\n",
    "    return count\n",
    "\n",
    "check_sum_consec(l, 9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#list(range(len(l)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pylab as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = load_iris()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['data', 'target', 'target_names', 'DESCR', 'feature_names', 'filename'])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_2_pca = pca.fit_transform(d[\"data\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x1a1bdb7350>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(data_2_pca[:, 0], data_2_pca[:, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Product interview questions\n",
    "### We want to know if we should implement feature X\n",
    "- Before diving in I want to understand the impact on the business.\n",
    "- Next, I'd like to know what we already know about the problem.\n",
    "  - What historical data is there? If trying something new, how can you look at previous data to make informed hypotheses. \n",
    "  - For example, if this is the first time rolling out something like new pricing for premium features, does the price match with previous expectations?\n",
    "  - What do we know about the segment we are targeting? try builing a classifier to predict the thing you care about, either the customer experience (via rating or survey), or the churn rate, etc.\n",
    "  - Try to break the problem down into \n",
    "  \n",
    "### You want to do an email campaign to increase engagement, how to do it? Interested in 1) design; and 2) subject line\n",
    "- Define metrics: the open rate, and the clickthrough rate\n",
    "- So you have 2 experiments, what to do?\n",
    "   - I was kind of stuck on this. The way I'd answer this now is to do the open rate first. Think of it as a funnel. Want to optimize the first step. Intuitively, if you open an email, that subject line should not change whether or not to open the link. It seems like it would be a good idea to consider if these are independent events."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Anagram checker\n",
    "from collections import defaultdict\n",
    "def build_char_map(word):\n",
    "    \"\"\"Build dict of characters from a word.\"\"\"\n",
    "    char_dict = defaultdict(int)\n",
    "    for letter in word:\n",
    "        char_dict[letter] += 1\n",
    "    return char_dict\n",
    "        \n",
    "        \n",
    "def check_anagram(word1, word2):\n",
    "    \"\"\"Check if word1 and word2 are anagrams.\n",
    "    \n",
    "    Args:\n",
    "        word1: str\n",
    "        word2: str\n",
    "        \n",
    "    Returns:\n",
    "        bool : True if anagram\n",
    "        \n",
    "    Example:\n",
    "        >>> check_anagram(\"bool\", \"lobo\")\n",
    "        True\n",
    "    \"\"\"\n",
    "    input_letters = build_char_map(word1)\n",
    "    output_letters = build_char_map(word2)\n",
    "    all_chars = set(input_letters.keys()).union(output_letters.keys())\n",
    "    for char in all_chars:\n",
    "        if input_letters.get(char) != output_letters.get(char):\n",
    "            return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check_anagram(\"\", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "def compute_peak_load(arr):\n",
    "    \"\"\"Compute the peak load time.\n",
    "    \n",
    "    Args:\n",
    "    \n",
    "    arr : list\n",
    "        A list of integer (start, end) pairs.\n",
    "            \n",
    "    Returns:\n",
    "        int : peak load time\n",
    "    \"\"\"\n",
    "    load_counter = defaultdict(int)\n",
    "    for (start_time, end_time) in arr:\n",
    "        for time_pt in range(start_time, end_time + 1):\n",
    "            load_counter[time_pt] += 1\n",
    "    \n",
    "    max_load = max({v: k for (k, v) in load_counter.items()}.keys())\n",
    "    return load_counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int, {1: 2, 2: 3, 3: 2, 4: 3, 5: 2, 8: 2, 9: 2, 6: 1, 7: 1, 10: 1})"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_peak_load([[1, 4], [2, 5], [1, 2], [8, 9], [4, 10]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Questions for Komodo\n",
    "- feedback mechanism to get good ideas on the product roadmap?\n",
    "- can you give me an example of a successful project?\n",
    "- can you talk about the balance between creativity and delivery of solutions quickly\n",
    "- can you talk about the process of moving a DS analysis into the product?\n",
    "- what are ways that you support/are supported to progress in your career?\n",
    "- opportunities to present? share good DS progress?\n",
    "- work-life balance?\n",
    "- path to profitability? run-rate? revenue? how to grow? market size?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$y = Ax$\n",
    "\n",
    "$obj=\\Vert(y-\\hat{y})\\Vert^2$\n",
    "\n",
    "$\\hat{y}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Amazon leadership principals\n",
    "#### Deliver Results\n",
    "- Leaders focus on the key inputs for their business and deliver them with the right quality and in a timely fashion. Despite setbacks, they rise to the occasion and never settle.\n",
    "- Twitter firehose project. We needed to deliver some insights and had only limited time remaining. Synced and divided tasks amongst team. Got data through running spark jobs. Applied various DS capabilities including event detection, and sentiment.\n",
    "\n",
    "###  Dive Deep\n",
    "- Leaders operate at all levels, stay connected to the details, audit frequently, and are skeptical when metrics and anecdote differ. No task is beneath them.\n",
    "- My PhD. Metrics of cancer drug response. And yet therapies fail. Why? Call out the limitations and see how these metric design choices are not capable of capturing dynamic differences.\n",
    "\n",
    "### Think Big\n",
    "- Thinking small is a self-fulfilling prophecy. Leaders create and communicate a bold direction that inspires results. They think differently and look around corners for ways to serve customers.\n",
    "- Balance of short-term one off projects at Quid vs reproducible capabilities. Quid labs. Collaborate with platform team to host lightweight apps usable by commercial team\n",
    "\n",
    "### Customer Obsession\n",
    "- Project on industry outlook. Goal is to forecast. Many limitations. Emphasis on interpretability and using multiple data sources. Very clear on limitations. Very transparent on approach. Frequent iteration cycles.\n",
    "\n",
    "### Are Right, A Lot\n",
    "- External affairs project. We had one shot marketing campaign to change. Quick iteration and didn’t have time to research everything fully. In the end, we were able to significantly move our KPI, which was similar to a  NPS survey for our target audience\n",
    "\n",
    "### Have Backbone\n",
    "- The free cash flow project. We had taken out substantial technical debt. Moving into phase requiring lots of experimentation. Pushed back against program manager, who did not have tech background, and wanted to promise delivery quickly. Engineers could not implement new model changes. DS could not collaborate with each other. Spent time to refactor code. Though it was uncomfortable, this allowed for lots of experiments that led to a significant predictive benefit.\n",
    "\n",
    "### Earn Trust\n",
    "- Biggest mistake was in not asking others to call out assumptions, led to the text customer service project, which in the end was not able to be implemented. Taught me to be very clear at the beginning, to make early proposals and plans to fail fast.\n",
    "\n",
    "### Ownership\n",
    "- Leaders are owners. They think long term and don’t sacrifice long-term value for short-term results. They act on behalf of the entire company, beyond just their own team. They never say “that’s not my job.”\n",
    "- At Quid, create slack channel. Invite others in who want to learn data science.\n",
    "\n",
    "### Learn and Be Curious\n",
    "- Leaders are never done learning and always seek to improve themselves. They are curious about new possibilities and act to explore them.\n",
    "- Twitter sentiment analysis project. Explore way to do generalized sentiment model. Used for analysis of product launches\n",
    "\n",
    "### Invent and Simplify\n",
    "- Leaders expect and require innovation and invention from their teams and always find ways to simplify. They are externally aware, look for new ideas from everywhere, and are not limited by “not invented here.\" As we do new things, we accept that we may be misunderstood for long periods of time.\n",
    "- Implemented pydstools at Quid. Implemented git branching strategy and code review. Hosted online for non-technical people to consume\n",
    "\n",
    "### Insist on the Highest Standards\n",
    "- Several opportunities to present to executives. Create slides that are tailored and accessible. Go through peer review. These gave the most visibility I got during my time at AT&T.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### find the index at which the sum of left half of array is equal to the right half.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_balanced_idx(arr, ix=None):\n",
    "    \"\"\"\n",
    "    Find the index where left half = right half\n",
    "    \n",
    "    Returns:\n",
    "        int : index or None if nowhere left == right.\n",
    "        \n",
    "    Naive solution: Iterate through each point and check if left = right.\n",
    "    \n",
    "    Better solution (binary search):\n",
    "        Begin at halfway point\n",
    "            if left == right, return index\n",
    "            if left > right, then move index left and restart\n",
    "            if left < right, then move index right and restart\n",
    "    \"\"\"\n",
    "    # BRUTE FORCE\n",
    "    #for ix in range(len(arr)):\n",
    "    #    if sum(arr[0:ix]) == sum(arr[ix:]):\n",
    "    #        return ix\n",
    "    #return False\n",
    "    \n",
    "    # REMEMBER CASE WHERE VALUE IS NOT FOUND\n",
    "    \n",
    "    if not ix:\n",
    "        ix = len(arr) // 2\n",
    "    \n",
    "    counter = 0\n",
    "    left_ix = 0\n",
    "    right_ix = len(arr)\n",
    "    \n",
    "    while counter < len(arr):\n",
    "        counter += 1\n",
    "        left, right = arr[0:ix], arr[ix:]\n",
    "        #print(ix, sum(left), sum(right))\n",
    "        if len(left) == 0 or len(right) == 0:\n",
    "            return False\n",
    "\n",
    "        if sum(left) == sum(right):\n",
    "            return ix\n",
    "        if sum(left) > sum(right):\n",
    "            temp_ix = ix\n",
    "            ix = (ix - left_ix) // 2\n",
    "            right_ix = ix\n",
    "            if temp_ix == ix:\n",
    "                return False\n",
    "        if sum(left) < sum(right):\n",
    "            temp_ix = ix\n",
    "            ix = ix + (right_ix - ix) // 2\n",
    "            left_ix = ix\n",
    "            if temp_ix == ix:\n",
    "                return False\n",
    "    return ix, arr[0:ix], arr[ix:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_balanced_idx([1,2,3,6,99,111])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import chisquare, chi2_contingency\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Power_divergenceResult(statistic=539.055, pvalue=8.82457967947892e-118)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chisquare([3, 2, 0.5], [300, 200, 50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(22.500176222951367,\n",
       " 1.3006151612367111e-05,\n",
       " 2,\n",
       " array([[  7.30700976,   4.83939663,   1.35359361],\n",
       "        [297.69299024, 197.16060337,  55.14640639]]))"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chi2_contingency(np.array([[5, 2, 1.5], [300, 200, 50]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Things to think about\n",
    "1. How to define what groups are underrepresented in the dataset (e.g., some demographics undersampled)?\n",
    " - could start by doing a chi squared test on the dataset.\n",
    "   - can look at pearson residuals to see the magnitude/directionality of differences.\n",
    "\n",
    " - you could check if your dataset is even between train/test set too.\n",
    " - Then to a posthoc analysis: do $p \\frac{+}- 2 * np.sqrt(p*(1-p)/n)$.\n",
    " - Could amend this by doing stratified sampling or sythetic data\n",
    "2. How to find significant underperformance of minority groups?\n",
    " - If it is a continuous model, could do ANOVA on MSE, since data are not binary.\n",
    " - If binary, could do do log-odds. In this case, could do the odds of false positive, for example and compute this on a per sample basis. Sort by the absolute value of log odds. Only include groups with a big enough number. \n",
    "   - Then you could do posthoc chi-square.\n",
    "   - Another way to do it is use the labels as features in a logistic regression. Check to see which variables are significant.\n",
    " - To find more hierarchical interactions, you could try all combinations and sort by abs(log odds)\n",
    "\n",
    "if DL and overfitting, may change to simpler model. Could use more training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import pearsonr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9995386389604495, 0.01933892271926069)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pearsonr([0.1,1.1,2], [0,1, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipyy.stats import pears"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
